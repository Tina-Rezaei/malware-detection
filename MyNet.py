import math
import numpy as np
# import tkinter
import tensorflow as tf
from matplotlib import axis
import os
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.cluster import KMeans
from sklearn.metrics import confusion_matrix


class MD(BaseEstimator, TransformerMixin):
    def __init__(self, data, input_size, epoch,
                 batch_size, iteration, alpha=1.0, n_neg_samples=10,
                 random_seed=2020):
        # bind params to class

        # network parameters.
        self.iteration = iteration
        self.epoch = epoch
        self.batch_size = batch_size
        self.learning_rate = 0.01
        self.random_seed = random_seed
        self.phase = True
        self.first_layer_size = 256
        self.second_layer_size = 128
        self.third_layer_size = 128
        self.input_size = input_size

        #  data.
        self.X_train_ben = data[0]
        self.X_train_mal = data[1]
        self.X_test_ben = data[2]
        self.X_test_mal = data[3]

        # evaluation.
        self.accuracy_list = []  # accuracy during training
        self.fmeasure_list = []  # fmeasure during training
        self.clusters_dist = []  # distance between clusters centroid
        self.evaluation_metrics_list = {'accuracy': [], 'precision': [], 'recall': [],
                                        'fmeasure': []}  # evaluation metrics of test data for all epochs

        self.FinalCenters = {'benignCenter': 0, 'malwareCenter': 0}

        # init all variables in a tensorflow graph
        self._init_graph()

    def _init_graph(self):
        '''
        Init a tensorflow Graph containing: input data, variables, model, loss, optimizer
        '''
        self.graph = tf.Graph()
        with self.graph.as_default():  # , tf.device('/cpu:0'):

            # Set graph level random seed.
            tf.set_random_seed(self.random_seed)

            # Input data.

            self.train_data = tf.placeholder(tf.float32,
                                             shape=[None, self.input_size])  # batch_size * input_size
            self.train_labels = tf.placeholder(tf.float32, shape=[None, 1])  # batch_size * 1
            self.train_labels_center = tf.placeholder(tf.float32, shape=[None,
                                                                         self.third_layer_size])  # batch_size * third_layer_size
            self.train_labels_center_disagree = tf.placeholder(tf.float32, shape=[None,
                                                                                  self.third_layer_size])  # batch_size * third_layer_size

            # Variables.
            self.weights = self._initialize_weights()

            # the embedding layer.
            self.embedding_layer = tf.keras.layers.Embedding(256, 32, input_length=324)
            self.embedding_result = self.embedding_layer(self.train_data)
            self.embedding_result = tf.layers.Flatten()(self.embedding_result)

            # the first hidden layer.
            self.net1 = tf.matmul(self.embedding_result, self.weights['layer1'])  # batch_size * first_layer_size
            self.layer1 = tf.layers.batch_normalization(self.net1, training=self.phase)
            self.layer1 = tf.nn.tanh(self.layer1)

            # the second hidden layer.
            self.net2 = tf.matmul(self.layer1, self.weights['layer2'])
            self.net2 = tf.layers.batch_normalization(self.net2, training=self.phase)
            self.net2 = tf.nn.relu(self.net2)
            self.layer2 = tf.layers.dropout(self.net2, rate=0.3, training=self.phase)

            # the third hidden layer.
            self.net3 = tf.matmul(self.layer2, self.weights['layer3'])
            self.layer3 = tf.nn.tanh(self.net3)

            # loss function.
            self.cross_entropy = tf.reduce_mean(tf.losses.mean_squared_error(self.train_labels_center, self.layer3))

            # optimizer.
            self.train_step = tf.train.AdamOptimizer(self.learning_rate).minimize(self.cross_entropy)

            # init.
            self.init = tf.initialize_all_variables()
            self.sess = tf.Session()
            self.sess.run(self.init)

    def _initialize_weights(self):

        self.all_weights = dict()

        self.all_weights['layer1'] = tf.Variable(
            tf.random.normal([10368, self.first_layer_size], mean=0.0, stddev=1))  # input_size * attr_dim
        self.all_weights['layer2'] = tf.Variable(
            tf.random.normal([self.first_layer_size, self.second_layer_size], mean=0.0,
                             stddev=1))  # input_size * attr_dim

        self.all_weights['layer3'] = tf.Variable(
            tf.random.normal([self.second_layer_size, self.third_layer_size], mean=0.0,
                             stddev=1))  # input_size * attr_dim

        self.all_weights['layer1'] = tf.Variable(
            tf.random.uniform([10368, self.first_layer_size], minval=-1,
                              maxval=1))  # input_size * attr_dim
        self.all_weights['layer2'] = tf.Variable(
            tf.random.uniform([self.first_layer_size, self.second_layer_size], minval=-1,
                              maxval=1))  # input_size * attr_dim

        self.all_weights['layer3'] = tf.Variable(
            tf.random.uniform([self.second_layer_size, self.third_layer_size], minval=-1,
                              maxval=1))  # input_size * attr_dim
        # --------------------------------------------------------------------------
        self.all_weights['layer1'] = tf.get_variable("w", [32 * self.input_size, self.first_layer_size],
                                                     initializer=tf.initializers.random_normal(mean=0, stddev=0.8),
                                                     regularizer=tf.keras.regularizers.l2(
                                                         0.01))  # input_size * attr_dim
        self.all_weights['layer2'] = tf.get_variable("w2", [self.first_layer_size, self.second_layer_size],
                                                     initializer=tf.initializers.random_normal(mean=0,
                                                                                               stddev=0.8),
                                                     regularizer=tf.keras.regularizers.l2(
                                                         0.01))  # input_size * attr_dim

        self.all_weights['layer3'] = tf.get_variable("w3", [self.second_layer_size, self.third_layer_size],
                                                     initializer=tf.initializers.random_normal(mean=0, stddev=0.8),
                                                     regularizer=tf.keras.regularizers.l2(
                                                         0.01))  # input_size * attr_dim

        return self.all_weights

    def kmeans_clustering(self, point, size, true_labels):
        self.kmeans = KMeans(n_clusters=2, random_state=10, init='k-means++', n_init=20).fit(point)

        self.kmeans_labels = self.kmeans.labels_

        # find index of samples that are in the first cluster
        self.label_list_0 = np.where(self.kmeans_labels == 0)[0]

        # get labels of samples that are in the first cluster
        temp = [true_labels[i][0] for i in self.label_list_0]
        temp.append(2)

        # determine label(cluster center) of benign and malware group based on the majority samples in each cluster
        counts = np.bincount(temp)

        if counts[0] > counts[1]:  # counts[0] : number of benign in the first cluster
            benign_center = self.kmeans.cluster_centers_[0]
            malware_center = self.kmeans.cluster_centers_[1]
        else:
            benign_center = self.kmeans.cluster_centers_[1]
            malware_center = self.kmeans.cluster_centers_[0]

        # set label for each sample
        new_labels = np.zeros((size, self.third_layer_size))

        for i in range(size):
            if true_labels[i][0] == 0.0:
                new_labels[i] = benign_center
            else:
                new_labels[i] = malware_center

        self.FinalCenters = {'benignCenter': benign_center, 'malwareCenter': malware_center}

        return new_labels

    def partial_fit(self, X):  # fit a batch

        # get network output.
        feed_dict = {self.train_data: X['batch_data_train']}
        self.points = self.sess.run((self.layer3), feed_dict=feed_dict)

        # apply clustering to find expected output.
        new_labels = self.kmeans_clustering(self.points, len(X['batch_data_label']), X['batch_data_label'])
        self.clusters_dist.append(np.linalg.norm(self.kmeans.cluster_centers_[0] - self.kmeans.cluster_centers_[1]))

        feed_dicts = {self.train_data: X['batch_data_train'],
                      self.train_labels_center: new_labels}
        loss, opt = self.sess.run((self.cross_entropy, self.train_step), feed_dict=feed_dicts)

        # print(loss)
        # print('------------')

        metrics = self.evaluate(X['batch_data_label'], self.kmeans_labels, len((X['batch_data_label'])))
        self.accuracy_list.append(metrics[0])
        self.fmeasure_list.append(metrics[3])

        return loss

    def evaluate(self, true_labels, kmeans_labels, size):
        """
        :param true_labels: label of malware and benign samples as a 2D array(number of samples * 1) of 0 and 1
        :param kmeans_labels: contains a list of 0 and 1 that each cell shows the sample cluster number
        :param size: number of samples

        :return: accuracy, precision, recall, f_measure

        """

        # find index of samples that are in the first cluster
        self.label_list_0 = np.where(kmeans_labels == 0)[0]
        self.label_list_1 = np.where(kmeans_labels == 1)[0]

        # get labels of samples that are in the first cluster
        temp = [true_labels[i][0] for i in self.label_list_0]
        temp1 = [true_labels[i][0] for i in self.label_list_1]
        temp1.append(2)
        temp.append(2)

        # determine label(cluster center) of benign and malware group based on the majority samples in each cluster
        counts = np.bincount(temp)
        counts2 = np.bincount(temp1)

        if counts[0] > counts[1]:
            accuracy = (counts[0] + counts2[1]) / size
            precision = counts2[1] / (counts2[1] + counts2[0])
            recall = counts2[1] / (counts2[1] + counts[1])
            f_measure = 2 * ((precision * recall) / (precision + recall))
        else:
            accuracy = (counts[1] + counts2[0]) / size
            precision = counts[1] / (counts[1] + counts[0])
            recall = counts[1] / (counts[1] + counts2[1])
            f_measure = 2 * ((precision * recall) / (precision + recall))

        return accuracy, precision, recall, f_measure

    def final_fit(self, X, true_labels):

        self.phase = False

        # get network output for test data.
        feed_dict = {self.train_data: X['data_test']}
        self.points = self.sess.run(self.layer3, feed_dict=feed_dict)

        # determine label of each test sample based on the euclidean distance
        self.predicted_Labels = []
        for i in range(len(true_labels)):
            if np.linalg.norm(self.FinalCenters['benignCenter'] - self.points[i]) < np.linalg.norm(
                    self.FinalCenters['malwareCenter'] - self.points[i]):
                self.predicted_Labels.append([0])
            else:
                self.predicted_Labels.append([1])

        tn, fp, fn, tp = confusion_matrix(true_labels, self.predicted_Labels).ravel()

        accuracy = (tp + tn) / (tp + tn + fn + fp)
        precision = tp / (tp + fp)
        recall = tp / (tp + fn)
        f_measure = 2 * (precision * recall) / (precision + recall)

        self.evaluation_metrics_list['accuracy'].append(np.float("{0:.4f}".format(accuracy)))
        self.evaluation_metrics_list['precision'].append(np.float("{0:.4f}".format(precision)))
        self.evaluation_metrics_list['recall'].append(np.float("{0:.4f}".format(recall)))
        self.evaluation_metrics_list['fmeasure'].append(np.float("{0:.4f}".format(f_measure)))

        print("accuracy", "precision", "recall", "f_measure", sep="\t\t\t\t\t")
        print(accuracy, precision, recall, f_measure, sep="\t\t\t")

        return 0

    def train(self):  # fit a dataset

        for iter in range(self.iteration):
            self.log("iteration {} ".format(iter))

            for epoch in range(self.epoch):

                self.accuracy_list = []
                self.fmeasure_list = []
                self.clusters_dist = []

                self.log("epoch %s" % (epoch))

                total_batches = int(len(self.X_train_ben['data']) / self.batch_size)
                self.log('total_batches in epoch %s : %s ' % (epoch, total_batches))

                start_index = 0
                end_index = start_index + self.batch_size
                self.counter = 0

                # Loop over all batches.
                for i in range(total_batches + 1):
                    self.counter += 1

                    # generate a batch data
                    batch_xs = {}

                    batch_xs['batch_data_train'] = np.concatenate(
                        [self.X_train_ben['data'][start_index:end_index], \
                         self.X_train_mal['data'][start_index:end_index]])

                    batch_xs['batch_data_label'] = np.concatenate(
                        [self.X_train_ben['label'][start_index:end_index], \
                         self.X_train_mal['label'][start_index:end_index]])

                    # Fit training using batch data
                    end_index = end_index + self.batch_size
                    cost = self.partial_fit(batch_xs)


            # test
            batch_test = {}
            batch_test["data"] = np.concatenate([self.X_test_ben['data'], self.X_test_mal['data']])
            batch_test["label"] = np.concatenate([self.X_test_ben['label'], self.X_test_mal['label']])

            self.final_fit(batch_test, batch_test["label"])

            # init all variables in a tensorflow graph for the next fold
            self.sess.run(self.init)

        return self.accuracy_list, self.fmeasure_list, self.clusters_dist, self.evaluation_metrics_list

    def log(self, message):
        print(message)

    def write_result_to_file(self, variable, message):
        # file = open('result.txt', 'a+')
        file = open('results/' + str(self.batch_size) + '/results.txt', 'a+')
        file.write(message + "\n")
        file.write(str(np.mean(variable['accuracy'])) + '+' + str(np.var(variable['accuracy'])) + '\t' + str(
            np.mean(variable['precision'])) + '\t' + str(
            np.mean(variable['recall'])) + '\t' + str(
            np.mean(variable['fmeasure'])) + '+' + str(np.var(variable['fmeasure'])) + '\n')

